import asyncio
import os
import pickle
from datetime import datetime

import pandas as pd
import plotly.express as px
import streamlit as st
from langgraph.pregel.io import AddableValuesDict

import tools
from mcp_tool_client import MCPClientFactory
from logger import Logger
from llm_chat_agent import LLMChatAgent, ToolConfig, CallAgentTool
from models import Models
from sql_executor_agent import SQLExecutorAgent
from sqllite_datasource import SqlLiteDatasource


OLLAMA_BASE_URL = st.secrets.get("OLLAMA_BASE_URL", "http://localhost:11434")
DEFAULT_AGENT_MODEL = Models.LLAMA_3B
DEFAULT_CODER_MODEL = Models.LLAMA_3B

HR_ASSISTANT_DESCRIPTION = """
Answers HR related questions.  
Call this whenever you need information about: departments and employees, including salaries
"""


def create_agent(model: Models):

    coder_model = Models.create_chat(
        Models(st.secrets.get("CODER_MODEL")) or DEFAULT_CODER_MODEL,
        base_url=OLLAMA_BASE_URL,
        temperature=0.3
    )
    chat_model = Models.create_chat(model, base_url=OLLAMA_BASE_URL, temperature=0.1)

    db_agent = LLMChatAgent(
        "DB Retrieval Agent", Logger.GREEN, coder_model,
        system_message="You are Database retrieval agent. " +
                       "Convert user query to SQL and call sql_exec_tool to execute SQL and get data. " +
                       "When generating final response, just return correct response generated by tool.",
        toolkit={"sql_exec_tool": ToolConfig(tools.sql_exec_tool, False, True)}
    )

    db_retrieval_tool = CallAgentTool(
        name="hr_assistant",
        description="""
        Answers HR related questions.
        Call this whenever you need information about: departments and employees, including salaries
        """,
        agent=db_agent
    )

    toolkit = {
        "conversational_response": ToolConfig(tools.conversational_response, True, True),
        "hr_assistant": ToolConfig(db_retrieval_tool, False, True),
        "command_exec_tool": ToolConfig(tools.command_exec_tool, False, False)
    }

    return LLMChatAgent("User Assistant Agent", Logger.BLUE, chat_model, toolkit=toolkit)


def init_session():
    if len(st.session_state.keys()) == 0:
        print("Initializing app session")
        st.session_state.messages = []
        st.session_state.models = (
            Models.LLAMA_3B.value,
            Models.QWEN25_CODER_7B.value,
            Models.QWEN3_8B.value,
            Models.MISTRAL.value,
            Models.GPT_4O_MINI.value
        )
        st.session_state.assistant = create_agent(
            Models(st.secrets.get("AGENT_MODEL")) or DEFAULT_AGENT_MODEL
        )
        coder_model = Models.create_chat(
            Models(st.secrets.get("CODER_MODEL")) or DEFAULT_CODER_MODEL,
            base_url=OLLAMA_BASE_URL,
            temperature=0.3
        )
        st.session_state.DBAgent = SQLExecutorAgent(SqlLiteDatasource("data/test-hr.db"), coder_model)


server_params_help = """
Enter either:  
â€¢ HTTP/HTTPS URL for SSE connection (e.g., http://localhost:8080/sse)  
or  
â€¢ JSON configuration for stdio connection. Sample format:  
```json  
{
"command": "uvx",  
"args": ["mcp-server-sqlite", "--db-path", "data/test-hr.db"],  
"env": {"DEBUG": "1"},  
"encoding": "utf-8"
}  
```
"""


def build_sidebar():
    with st.sidebar:
        st.title("Chat-Driven Tool Runner")

        option = st.selectbox(
            "Model:",
            st.session_state.models,
            index=st.session_state.models.index(st.session_state.assistant.get_llm_name())
        )

        if option != st.session_state.assistant.get_llm_name():
            st.session_state.assistant.set_model(Models.create_chat(Models(option), base_url=OLLAMA_BASE_URL, temperature=0.3))

        if st.toggle("Lang Chain graph"):
            st.image(st.session_state.assistant.get_graph_image())

        toolkit = st.session_state.assistant.get_toolkit()

        with st.expander("Build-in tools:", expanded=True):
            for name, tool in toolkit.items():
                if not tool.is_mcp_tool:
                    tool.enabled = st.toggle(name, tool.enabled, help=tool.function.description)

        with st.expander("MCP tools:", expanded=True):

            server_params = st.text_area("MCP server parameters:", help=server_params_help)

            if st.button("Attach", type="primary") and server_params:
                with st.spinner("Loading..."):
                    try:
                        mcp_client = MCPClientFactory.create_from(server_params)

                        try:
                            loop = asyncio.ProactorEventLoop()
                            asyncio.set_event_loop(loop)
                            mcp_toolkit = loop.run_until_complete(mcp_client.get_toolkit())
                        finally:
                            loop.close()

                        toolkit.update(mcp_toolkit)
                    except ValueError as e:
                        st.error(e)
            col1, col2 = st.columns(2)
            col1.write("tool")
            col2.write("auto-approval")
            col_tool, col_cb = st.columns([3, 1])
            with col_tool:
                for name, tool in toolkit.items():
                    if tool.is_mcp_tool:
                        tool.enabled = st.toggle(name, tool.enabled, help=tool.function.description)

            with col_cb:
                for name, tool in toolkit.items():
                    if tool.is_mcp_tool:
                        tool.auto_exec = st.checkbox("-", tool.auto_exec, key=f"approval-cb-{name}",  label_visibility="hidden")

        pkl_files = get_pkl_files()
        if pkl_files:
            selected_file = st.selectbox(
                "Select history file:",
                options=pkl_files,
                key="history_file"
            )

        load_col, save_col = st.columns(2, vertical_alignment="bottom")
        if pkl_files:
            if load_col.button("Load history", key="load_history_button", use_container_width=True):
                try:
                    with open(os.path.join("data", selected_file), 'rb') as f:
                        data = pickle.load(f)
                        st.session_state.messages = data["chat_messages"]
                        st.session_state.assistant.set_history(data["agent_messages"])
                    st.rerun()
                except Exception as e:
                    st.error(f"unable to load history file: {e}")

        if save_col.button("Save history", key="save_history_button", use_container_width=True):
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"data/history-{timestamp}.pkl"
            data = {
                "chat_messages": st.session_state.messages,
                "agent_messages":  st.session_state.assistant.get_history()
            }
            with open(filename, 'wb') as f:
                pickle.dump(data, f)
            st.rerun()

        if st.button("About", key="about_button", use_container_width=True):
            show_about()


def get_pkl_files(directory="data"):
    """Get all .pkl files from the specified directory"""
    if not os.path.exists(directory):
        return []
    return [f for f in os.listdir(directory) if f.endswith('.pkl')]

def run_command(state):
    placeholder = st.empty()
    with placeholder.container():
        with st.spinner("Thinking..."):
            output = tools.command_exec_tool(state['command'])
            state['output'] = output
    with placeholder.container():
        st.markdown(f"```\n\n{output}\n\n```")


def draw_pie_chart(id: str, df: pd.DataFrame):
    # Create the pie chart using Plotly Express
    fig = px.pie(df, values=df.columns[1], names=df.columns[0], title='Pie Chart')
    # Display the chart in Streamlit
    st.plotly_chart(fig, key="pie-chart-" + id)


def artifacts_widget(artifacts, level: str = ""):
    i = 0
    for artifact in artifacts:
        i += 1
        st.markdown(f":orange-badge[**Iteration {level}{i}**]")
        if artifact.get("input"):
            st.markdown(f"**{artifact.get('input')}**")

        result = artifact.get("result")

        if type(result) is pd.DataFrame:
            st.dataframe(artifact.get("result"), use_container_width=False)
            # if st.button("pie chart", key="pie-chart-btn-" + id):
            #     message["draw_pie_chart"] = True
        elif type(result) is AddableValuesDict:
            if sub_artifacts := result.get("artifacts"):
                artifacts_widget(sub_artifacts, f"{level}{i}.")
            st.markdown(result['output'])
        else:
            st.write(f"```\n{artifact.get('result')}\n```")


def print_assistant_response(message: dict):
    llm_response = message['content']
    id = message['id']

    if llm_response.get('command'):
        st.markdown(f"```\n\n{llm_response['output']}\n\n```")
        with st.expander("Response details:", expanded=True):
            st.write(f"`{llm_response['command']}`")
            st.write(llm_response["info"])

            if st.button("run it", key="btn-" + id):
                message["run_command"] = True

            if message.get("run_command", False):
                run_command(llm_response)
                message["run_command"] = False

    elif llm_response.get('sql'):
        st.markdown(f"```\n\n{llm_response['result']['message']}\n\n```")
        dataframe: pd.DataFrame = llm_response['result']['dataframe']
        st.dataframe(dataframe, use_container_width=True)

        with st.expander("See details"):
            st.write(f"`{llm_response['sql']}`")
            st.write(llm_response["info"])

            if st.button("pie chart", key="pie-chart-btn-" + id):
                message["draw_pie_chart"] = True

        if message.get("draw_pie_chart", False):
            draw_pie_chart(id, dataframe)

    else:
        response = tools.parse_response(llm_response['output'])
        artifacts = llm_response.get('artifacts')
        if artifacts or response.get("model-thoughts"):
            with st.expander("See details"):
                if artifacts:
                    artifacts_widget(artifacts)
                if response.get("model-thoughts"):
                    st.info(f"ðŸ’­ :orange-badge[**Model thoughts:**]\n\n{response['model-thoughts']}")                    
        st.write(response['response-text'])
        #st.write(f"```\n{llm_response['output']}\n```")


def build_chat_page():
    st.title("Chat-Driven Tool Runner")
    st.text("This AI chat bot provides a natural language interface for system commands. ")
    st.info("Preface your message with `$` to execute a command automatically and view output in real time.", icon=":material/info:")

    with st.expander("System instruction message"):
        st.write(st.session_state.assistant.get_system_instruction())

    # Display chat messages from history on app rerun
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            if message["role"] == "assistant":
                print_assistant_response(message)
            else:
                st.markdown(message["content"])

    # React to user input
    if human_message := st.chat_input("Write your question"):

        # Display user message in chat message container
        with st.chat_message("user"):
            st.markdown(human_message)
        # Add user message to chat history
        st.session_state.messages.append(
            {"id": tools.generate_random_string(16), "role": "user", "content": human_message}
        )

        # Display assistant response in chat message container
        with st.chat_message("assistant"):

            execute_command = False
            agent = st.session_state.assistant
            # response = st.write_stream(response_generator())
            placeholder = st.empty()
            with placeholder.container():
                with st.spinner("Thinking..."):
                    if human_message.startswith("db>"):
                        agent = st.session_state.DBAgent
                        execute_command = True
                        query = human_message[3:]
                    elif human_message.startswith("$"):
                        execute_command = True
                        query = human_message[1:]
                    else:
                        query = human_message

                    llm_response = agent.invoke(query, execute_command)

            with placeholder.container():
                message = {"id": tools.generate_random_string(16), "role": "assistant", "content": llm_response}
                print_assistant_response(message)

        # Add assistant response to chat history
        st.session_state.messages.append(message)


def config():
    st.set_page_config(
        layout="wide",
        page_title="Chat-Driven Tool Runner",
        menu_items={
            'About': ("A Streamlit-based chat app that uses a local LLM to interpret user requests, "
                      "generate commands, and automatically execute them "
                      "- offering a simple natural language interface for system actions.")
        })


@st.dialog("About", width="large")
def show_about():
    # Read the content of README.md
    with open('README.md', 'r') as file:
        readme_content = file.read()

    # Display the content in the dialog
    st.markdown(readme_content)


def run():
    config()
    init_session()
    build_sidebar()
    build_chat_page()


if __name__ == '__main__':
    run()
